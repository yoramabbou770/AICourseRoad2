{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "14aKcFcqXqxG6bTWNm2XkP-FuP2qi0-8L",
      "authorship_tag": "ABX9TyPqnVUE+4g15s5MPyzrVVdc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoramabbou770/AICourseRoad2/blob/main/Task7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip my zip folder, the path is : /content/drive/MyDrive/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "id": "cqgqU2qXF0E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/drive/MyDrive/cats_and_dogs_filtered.zip'\n",
        "\n",
        "# Directory to extract the contents\n",
        "extracted_dir = '/content/cats_and_dogs_filtered'\n",
        "\n",
        "# Check if the directory already exists, if not, create it\n",
        "if not os.path.exists(extracted_dir):\n",
        "    os.makedirs(extracted_dir)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "# Display the list of files and directories in the extracted folder\n",
        "print(\"Contents of the extracted folder:\")\n",
        "print(os.listdir(extracted_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQZanBOvescJ",
        "outputId": "24ee77ef-457f-454e-bf99-81f202e21bca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the extracted folder:\n",
            "['cats_and_dogs_filtered']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give me in python and Keras for google colab a model that classify if its a dog or a cat, use the VGG16 model, freeze the top layers, just let fit from the last convolution layer to the end classification layer included.\n",
        "the path are : /content/cats_and_dogs_filtered/cats_and_dogs_filtered/train and /content/cats_and_dogs_filtered/cats_and_dogs_filtered/validation"
      ],
      "metadata": {
        "id": "bzn5VL9GGHBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16\n",
        "from keras import models, layers, optimizers\n",
        "\n",
        "# Set the path to your extracted dataset\n",
        "train_dataset_path = '/content/cats_and_dogs_filtered/cats_and_dogs_filtered/train'\n",
        "validation_dataset_path = '/content/cats_and_dogs_filtered/cats_and_dogs_filtered/validation'\n",
        "\n",
        "# Define parameters\n",
        "img_width, img_height = 224, 224\n",
        "input_shape = (img_width, img_height, 3)\n",
        "batch_size = 32\n",
        "\n",
        "# Create VGG16 base model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Freeze the convolutional layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model for classification\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add the VGG16 convolutional base\n",
        "model.add(base_model)\n",
        "\n",
        "# Add Flatten layer to match the PyTorch model's input shape\n",
        "model.add(layers.Flatten(input_shape=(7, 7, 512)))\n",
        "\n",
        "# Add fully connected layers matching the PyTorch model\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # 1 neuron for binary classification (cat or dog)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation and preparation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',  # binary classification\n",
        "    classes=['cats', 'dogs'],  # class labels\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    classes=['cats', 'dogs'],\n",
        ")\n",
        "\n",
        "# Calculate steps_per_epoch and validation_steps\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = validation_generator.samples // batch_size\n",
        "\n",
        "# Train the model with validation data\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps\n",
        ")"
      ],
      "metadata": {
        "id": "wXWYqy7kEBJS",
        "outputId": "97a4e5fc-8f25-4224-c522-b2f18684ca29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "62/62 [==============================] - 35s 545ms/step - loss: 0.6286 - accuracy: 0.6712 - val_loss: 0.3480 - val_accuracy: 0.8478\n",
            "Epoch 2/5\n",
            "62/62 [==============================] - 32s 510ms/step - loss: 0.4055 - accuracy: 0.8095 - val_loss: 0.3049 - val_accuracy: 0.8669\n",
            "Epoch 3/5\n",
            "62/62 [==============================] - 34s 545ms/step - loss: 0.3341 - accuracy: 0.8476 - val_loss: 0.2363 - val_accuracy: 0.9032\n",
            "Epoch 4/5\n",
            "62/62 [==============================] - 36s 584ms/step - loss: 0.3073 - accuracy: 0.8674 - val_loss: 0.2260 - val_accuracy: 0.9083\n",
            "Epoch 5/5\n",
            "62/62 [==============================] - 32s 519ms/step - loss: 0.2962 - accuracy: 0.8613 - val_loss: 0.2103 - val_accuracy: 0.9113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without freeze the top layers"
      ],
      "metadata": {
        "id": "hKtK7VGyJxKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16\n",
        "from keras import models, layers, optimizers\n",
        "\n",
        "# Set the path to your extracted dataset\n",
        "train_dataset_path = '/content/cats_and_dogs_filtered/cats_and_dogs_filtered/train'\n",
        "validation_dataset_path = '/content/cats_and_dogs_filtered/cats_and_dogs_filtered/validation'\n",
        "\n",
        "# Define parameters\n",
        "img_width, img_height = 224, 224\n",
        "input_shape = (img_width, img_height, 3)\n",
        "batch_size = 32\n",
        "\n",
        "# Create VGG16 base model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Create a new model for classification\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add the VGG16 convolutional base\n",
        "model.add(base_model)\n",
        "\n",
        "# Add Flatten layer to match the PyTorch model's input shape\n",
        "model.add(layers.Flatten(input_shape=(7, 7, 512)))\n",
        "\n",
        "# Add fully connected layers matching the PyTorch model\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # 1 neuron for binary classification (cat or dog)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation and preparation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',  # binary classification\n",
        "    classes=['cats', 'dogs'],  # class labels\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    classes=['cats', 'dogs'],\n",
        ")\n",
        "\n",
        "# Calculate steps_per_epoch and validation_steps\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = validation_generator.samples // batch_size\n",
        "\n",
        "# Train the model with validation data\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps\n",
        ")"
      ],
      "metadata": {
        "id": "xCMNSN5PHUWn",
        "outputId": "4c8d5915-67e5-4533-e3a0-20d1aae9e44b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "62/62 [==============================] - 67s 820ms/step - loss: 0.5960 - accuracy: 0.6712 - val_loss: 0.2774 - val_accuracy: 0.8982\n",
            "Epoch 2/5\n",
            "62/62 [==============================] - 42s 659ms/step - loss: 0.2546 - accuracy: 0.8943 - val_loss: 0.1074 - val_accuracy: 0.9607\n",
            "Epoch 3/5\n",
            "62/62 [==============================] - 43s 695ms/step - loss: 0.1369 - accuracy: 0.9522 - val_loss: 0.0827 - val_accuracy: 0.9657\n",
            "Epoch 4/5\n",
            "62/62 [==============================] - 42s 679ms/step - loss: 0.1135 - accuracy: 0.9517 - val_loss: 0.0710 - val_accuracy: 0.9798\n",
            "Epoch 5/5\n",
            "62/62 [==============================] - 41s 654ms/step - loss: 0.0769 - accuracy: 0.9670 - val_loss: 0.0691 - val_accuracy: 0.9718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give me the equivalent code for the PyTorch model"
      ],
      "metadata": {
        "id": "lrPhDGZqHWku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        self.data = ImageFolder(data_path, transform=transform)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Define the PyTorch model with frozen top layers\n",
        "class CustomVGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomVGG16, self).__init__()\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Freeze the top layers\n",
        "        for param in vgg16.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Extract the layers from the last convolutional layer to the end of the classifier\n",
        "        classifier_layers = list(vgg16.classifier.children())[-6:]\n",
        "\n",
        "        self.features = vgg16.features\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),  # Adjusted input size to match flattened output\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Set the path to your extracted dataset\n",
        "train_dataset_path = '/content/cats_and_dogs_filtered/cats_and_dogs_filtered/train'\n",
        "validation_dataset_path = '/content/cats_and_dogs_filtered/cats_and_dogs_filtered/validation'\n",
        "\n",
        "# Define parameters\n",
        "img_width, img_height = 224, 224\n",
        "input_shape = (3, img_width, img_height)  # PyTorch uses (channels, height, width)\n",
        "batch_size = 32\n",
        "\n",
        "# Define data transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((img_width, img_height)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "validation_transform = transforms.Compose([\n",
        "    transforms.Resize((img_width, img_height)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create custom datasets and data loaders\n",
        "train_dataset = CustomDataset(train_dataset_path, transform=train_transform)\n",
        "validation_dataset = CustomDataset(validation_dataset_path, transform=validation_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Create the PyTorch model\n",
        "pytorch_model = CustomVGG16()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.RMSprop(pytorch_model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    pytorch_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = pytorch_model(inputs)\n",
        "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    average_loss = running_loss / len(train_loader)\n",
        "    print(f'Training Loss: {average_loss}')\n",
        "\n",
        "# Validation loop\n",
        "pytorch_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(validation_loader, desc='Validation'):\n",
        "        outputs = pytorch_model(inputs)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.float().view(-1, 1)).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Validation Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ae5Fkgzmz4p",
        "outputId": "be99f7fd-cb51-4fd9-c69e-b69f209a3373"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 63/63 [21:08<00:00, 20.13s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.23582229550395692\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 63/63 [20:54<00:00, 19.92s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.115681962716201\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 63/63 [20:36<00:00, 19.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.11722880504315808\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 63/63 [20:39<00:00, 19.67s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.11370683402296096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 63/63 [21:10<00:00, 20.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.10982418229566916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 32/32 [09:47<00:00, 18.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}